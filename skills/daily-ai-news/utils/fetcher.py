"""å†…å®¹æŠ“å–æ¨¡å— - ä»ç§‘æŠ€åª’ä½“ RSS æŠ“å– AI ç›¸å…³æ–°é—»"""
import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from translator import MockTranslator


class TechNewsFetcher:
    """ç§‘æŠ€æ–°é—»å†…å®¹æŠ“å–å™¨"""
    """ç§‘æŠ€æ–°é—»å†…å®¹æŠ“å–å™¨"""

    def __init__(self):
        # ç§‘æŠ€åª’ä½“ RSS æº
        self.rss_sources = [
            {
                "name": "TechCrunch",
                "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "category": "ç§‘æŠ€åª’ä½“"
            },
            {
                "name": "The Verge",
                "url": "https://www.theverge.com/rss/artificial-intelligence/index.xml",
                "category": "ç§‘æŠ€åª’ä½“"
            },
            {
                "name": "VentureBeat",
                "url": "https://venturebeat.com/category/ai/feed/",
                "category": "ç§‘æŠ€åª’ä½“"
            },
            {
                "name": "MIT Technology Review",
                "url": "https://www.technologyreview.com/feed/",
                "category": "ç§‘æŠ€åª’ä½“"
            },
            {
                "name": "AI News",
                "url": "https://artificialintelligence-news.com/feed/",
                "category": "AI ä¸“ä¸š"
            }
        ]

        # AI ç›¸å…³å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        self.ai_keywords = [
            "AI", "artificial intelligence", "äººå·¥æ™ºèƒ½", "machine learning", "æœºå™¨å­¦ä¹ ",
            "deep learning", "æ·±åº¦å­¦ä¹ ", "neural network", "ç¥ç»ç½‘ç»œ", "LLM", "GPT",
            "Claude", "ChatGPT", "openai", "google deepmind", "gemini", "copilot",
            "midjourney", "stable diffusion", "diffusion model", "transformer",
            "generative", "ç”Ÿæˆå¼", "reinforcement learning", "å¼ºåŒ–å­¦ä¹ ",
            "computer vision", "nlp", "natural language processing", "robotics",
            "autonomous", "automation", "æ™ºèƒ½", "å¤§æ¨¡å‹", "agentic", "å¤šæ¨¡æ€"
        ]

        # è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }

        # ç¿»è¯‘å™¨
        self.translator = MockTranslator()

    def _is_ai_related(self, text: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ AI ç›¸å…³"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.ai_keywords)

    def _fetch_rss(self, url: str) -> Optional[BeautifulSoup]:
        """è·å– RSS feed"""
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, "xml")
        except requests.exceptions.RequestException as e:
            print(f"   âš ï¸  è·å– RSS å¤±è´¥: {url[:50]}... - {e}")
            return None

    def _parse_rss_item(self, item, source_name: str) -> Optional[Dict]:
        """è§£æ RSS å•ä¸ªæ¡ç›®"""
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            title = item.find("title")
            link = item.find("link")
            description = item.find("description")
            pub_date = item.find("pubDate")
            author = item.find("author") or item.find("dc:creator")
            category = item.find("category")

            if not title or not link:
                return None

            title_text = title.get_text(strip=True)
            link_text = link.get_text(strip=True) or link.get("href", "")

            # æ¸…ç†æè¿°ï¼ˆç§»é™¤ HTML æ ‡ç­¾ï¼‰
            desc_text = ""
            if description:
                desc_soup = BeautifulSoup(description.get_text(), "html.parser")
                desc_text = desc_soup.get_text(strip=True)[:500]  # é™åˆ¶é•¿åº¦

            # æ£€æŸ¥æ˜¯å¦ä¸ AI ç›¸å…³
            if not self._is_ai_related(title_text + " " + desc_text):
                return None

            # è§£æå‘å¸ƒæ—¶é—´
            pub_time = pub_date.get_text(strip=True) if pub_date else ""
            try:
                pub_dt = datetime.strptime(pub_time, "%a, %d %b %Y %H:%M:%S %z")
                pub_dt = pub_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            except:
                pub_dt = pub_time

            # ç”Ÿæˆä¸­æ–‡ç¿»è¯‘
            translations = self.translator.generate_chinese_translation(title_text, desc_text)

            return {
                "id": link_text.split("/")[-1][:50],
                "title": title_text,
                "title_cn": translations["title_cn"],
                "text": f"{title_text}\n\n{desc_text}",
                "text_cn": translations["text_cn"],
                "author": {
                    "id": source_name,
                    "username": source_name.lower().replace(" ", "_"),
                    "name": author.get_text(strip=True) if author else source_name,
                    "avatar": ""
                },
                "metrics": {
                    "like_count": 0,
                    "retweet_count": 0,
                    "reply_count": 0
                },
                "created_at": pub_dt,
                "url": link_text,
                "source": source_name,
                "category_text": category.get_text(strip=True) if category else ""
            }
        except Exception as e:
            print(f"   âš ï¸  è§£ææ¡ç›®å¤±è´¥: {e}")
            return None

    def fetch_by_rss(self, date: datetime = None) -> List[Dict]:
        """ä½¿ç”¨ RSS æŠ“å–å†…å®¹"""
        date = date or datetime.now()
        start_date = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = start_date + timedelta(days=1)

        all_articles = []

        print(f"   å¼€å§‹æŠ“å– {len(self.rss_sources)} ä¸ª RSS æº...")

        for source in self.rss_sources:
            print(f"   ğŸ“¡ {source['name']}: ", end="", flush=True)
            soup = self._fetch_rss(source["url"])

            if not soup:
                print("å¤±è´¥")
                continue

            items = soup.find_all("item")
            count = 0

            for item in items:
                article = self._parse_rss_item(item, source["name"])
                if article:
                    all_articles.append(article)
                    count += 1

            print(f"æˆåŠŸï¼Œè·å– {count} ç¯‡")

        return all_articles

    def fetch_mock(self, date: datetime = None) -> List[Dict]:
        """æ¨¡æ‹ŸæŠ“å–ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")

        # æ¨¡æ‹Ÿæ•°æ®
        mock_articles = [
            {
                "id": f"1_{date_str}",
                "title": "GPT-5 Leaks: OpenAI's Next Model to Feature Real-Time Multimodal Understanding",
                "title_cn": "GPT-5 æ›å…‰ï¼šOpenAI ä¸‹ä¸€ä»£æ¨¡å‹å°†å…·å¤‡å®æ—¶å¤šæ¨¡æ€ç†è§£èƒ½åŠ›",
                "text": "Reports indicate that GPT-5 will possess real-time multimodal understanding capabilities. This breakthrough could revolutionize how AI interacts with the world.",
                "text_cn": "æ¶ˆæ¯ç§° GPT-5 å°†å…·å¤‡å®æ—¶å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€çªç ´å¯èƒ½å½»åº•æ”¹å˜ AI ä¸ä¸–ç•Œçš„äº’åŠ¨æ–¹å¼ã€‚é¢„è®¡å°†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒè¯†åˆ«å’ŒéŸ³é¢‘ç†è§£æ–¹é¢å–å¾—é‡å¤§è¿›å±•ã€‚",
                "author": {"id": "TechCrunch", "username": "techcrunch", "name": "TechCrunch", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T10:30:00Z",
                "url": "https://techcrunch.com/2026/02/11/gpt5-leaks/",
                "source": "TechCrunch",
                "category_text": "AI"
            },
            {
                "id": f"2_{date_str}",
                "title": "Claude Sonnet 4.5's Code Understanding Boosts Developer Productivity by 200%",
                "title_cn": "Claude Sonnet 4.5 çš„ä»£ç ç†è§£èƒ½åŠ›ä½¿å¼€å‘è€…ç”Ÿäº§åŠ›æå‡ 200%",
                "text": "Developers report significant productivity gains using Claude Sonnet 4.5 for coding tasks. The model's ability to understand and write complex code has improved dramatically.",
                "text_cn": "å¼€å‘è€…æŠ¥å‘Šä½¿ç”¨ Claude Sonnet 4.5 è¿›è¡Œç¼–ç ä»»åŠ¡æ—¶ç”Ÿäº§åŠ›æ˜¾è‘—æå‡ã€‚è¯¥æ¨¡å‹ç†è§£å’Œç¼–å†™å¤æ‚ä»£ç çš„èƒ½åŠ›å¤§å¹…æå‡ï¼Œå¤§å¤§å‡å°‘äº†å¼€å‘æ—¶é—´å’Œé”™è¯¯ç‡ã€‚",
                "author": {"id": "The Verge", "username": "theverge", "name": "The Verge", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T09:15:00Z",
                "url": "https://www.theverge.com/2026/02/11/claude-sonnet-4-5-coding",
                "source": "The Verge",
                "category_text": "Technology"
            },
            {
                "id": f"3_{date_str}",
                "title": "Stable Diffusion 3.0 Released with Major Quality Improvements",
                "title_cn": "Stable Diffusion 3.0 å‘å¸ƒï¼Œç”»è´¨æ˜¾è‘—æå‡",
                "text": "Stability AI has released Stable Diffusion 3.0 with significant improvements in image quality and generation speed. The update includes new features for text rendering and composition.",
                "text_cn": "Stability AI å‘å¸ƒäº† Stable Diffusion 3.0ï¼Œåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚æ›´æ–°åŒ…æ‹¬æ–‡æœ¬æ¸²æŸ“å’Œæ„å›¾çš„æ–°åŠŸèƒ½ï¼Œä¸ºåˆ›ä½œè€…æä¾›äº†æ›´å¼ºå¤§çš„å·¥å…·ã€‚",
                "author": {"id": "VentureBeat", "username": "venturebeat", "name": "VentureBeat", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T11:45:00Z",
                "url": "https://venturebeat.com/2026/02/11/stable-diffusion-3-0/",
                "source": "VentureBeat",
                "category_text": "AI"
            },
            {
                "id": f"4_{date_str}",
                "title": "Breakthrough in LLM Inference Cost Optimization",
                "title_cn": "LLM æ¨ç†æˆæœ¬ä¼˜åŒ–å–å¾—çªç ´",
                "text": "New quantization techniques enable small language models to perform at the level of much larger ones. This could democratize access to powerful AI.",
                "text_cn": "æ–°çš„é‡åŒ–æŠ€æœ¯ä½¿å°å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°æ›´å¤§è§„æ¨¡æ¨¡å‹çš„æ•ˆæœã€‚è¿™å¯èƒ½æ™®åŠå¯¹å¼ºå¤§ AI çš„è®¿é—®ï¼Œé™ä½äººå·¥æ™ºèƒ½ä½¿ç”¨æˆæœ¬ã€‚",
                "author": {"id": "MIT Technology Review", "username": "mit_tech_review", "name": "MIT Technology Review", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T08:20:00Z",
                "url": "https://www.technologyreview.com/2026/02/11/llm-optimization/",
                "source": "MIT Technology Review",
                "category_text": "Research"
            },
            {
                "id": f"5_{date_str}",
                "title": "Google Gemini 2.5 Introduces Advanced Code Execution Capabilities",
                "title_cn": "Google Gemini 2.5 å¼•å…¥é«˜çº§ä»£ç æ‰§è¡Œèƒ½åŠ›",
                "text": "Google's latest Gemini model can now execute Python code directly, providing developers with a powerful tool for data analysis and prototyping.",
                "text_cn": "Google çš„æœ€æ–° Gemini æ¨¡å‹ç°åœ¨å¯ä»¥ç›´æ¥æ‰§è¡Œ Python ä»£ç ï¼Œä¸ºå¼€å‘è€…æä¾›æ•°æ®åˆ†æå’ŒåŸå‹è®¾è®¡çš„å¼ºå¤§å·¥å…·ï¼Œå¤§å¤§æé«˜äº†å¼€å‘æ•ˆç‡ã€‚",
                "author": {"id": "AI News", "username": "ai_news", "name": "AI News", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T12:00:00Z",
                "url": "https://artificialintelligence-news.com/2026/02/11/google-gemini-2-5/",
                "source": "AI News",
                "category_text": "AI"
            },
            {
                "id": f"6_{date_str}",
                "title": "Agentic AI Systems: The Next Frontier in Artificial Intelligence",
                "title_cn": "æ™ºèƒ½ä»£ç† AI ç³»ç»Ÿï¼šäººå·¥æ™ºèƒ½çš„ä¸‹ä¸€ä¸ªå‰æ²¿",
                "text": "Research shows that agentic AI systems capable of autonomous planning and execution are becoming increasingly sophisticated. This shift could transform enterprise automation.",
                "text_cn": "ç ”ç©¶è¡¨æ˜ï¼Œèƒ½å¤Ÿè‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œçš„æ™ºèƒ½ä»£ç† AI ç³»ç»Ÿæ­£å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚è¿™ç§è½¬å˜å¯èƒ½ä¼šæ”¹å˜ä¼ä¸šè‡ªåŠ¨åŒ–ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå†³ç­–è´¨é‡ã€‚",
                "author": {"id": "The Verge", "username": "theverge", "name": "The Verge", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T13:30:00Z",
                "url": "https://www.theverge.com/2026/02/11/agentic-ai-systems/",
                "source": "The Verge",
                "category_text": "AI"
            },
            {
                "id": f"7_{date_str}",
                "title": "Multimodal AI Models Achieve Human-Level Performance on Complex Tasks",
                "title_cn": "å¤šæ¨¡æ€ AI æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¾¾åˆ°äººç±»æ°´å¹³è¡¨ç°",
                "text": "New benchmarks show that the latest multimodal AI models can match or exceed human performance on complex reasoning tasks that require understanding text, images, and audio simultaneously.",
                "text_cn": "æœ€æ–°çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæœ€æ–°çš„å¤šæ¨¡æ€ AI æ¨¡å‹å¯ä»¥åœ¨éœ€è¦åŒæ—¶ç†è§£æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¶Šäººç±»è¡¨ç°ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½çš„é‡å¤§è¿›æ­¥ã€‚",
                "author": {"id": "TechCrunch", "username": "techcrunch", "name": "TechCrunch", "avatar": ""},
                "metrics": {"like_count": 0, "retweet_count": 0, "reply_count": 0},
                "created_at": f"{date_str}T14:15:00Z",
                "url": "https://techcrunch.com/2026/02/11/multimodal-benchmark/",
                "source": "TechCrunch",
                "category_text": "AI"
            }
        ]

        return mock_articles

    def fetch(self, date: datetime = None, use_rss: bool = False) -> List[Dict]:
        """
        æŠ“å–å†…å®¹

        Args:
            date: ç›®æ ‡æ—¥æœŸ
            use_rss: æ˜¯å¦ä½¿ç”¨ RSSï¼ˆFalse åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
        """
        if use_rss:
            return self.fetch_by_rss(date)
        else:
            return self.fetch_mock(date)

    def save_to_file(self, articles: List[Dict], date: datetime = None) -> str:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")

        # ç¡®ä¿ data ç›®å½•å­˜åœ¨
        data_dir = os.path.join(os.path.dirname(__file__), "data")
        os.makedirs(data_dir, exist_ok=True)

        filepath = os.path.join(data_dir, f"articles_{date_str}.json")
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump({
                "date": date_str,
                "count": len(articles),
                "articles": articles
            }, f, ensure_ascii=False, indent=2)

        return filepath


# ä¿ç•™æ—§çš„ç±»åä½œä¸ºåˆ«åï¼Œç¡®ä¿å…¼å®¹æ€§
TwitterFetcher = TechNewsFetcher


if __name__ == "__main__":
    # æµ‹è¯•
    fetcher = TechNewsFetcher()
    print("ä½¿ç”¨ RSS æŠ“å–...")
    articles = fetcher.fetch(use_rss=True)
    print(f"æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    filepath = fetcher.save_to_file(articles)
    print(f"ä¿å­˜åˆ°: {filepath}")
